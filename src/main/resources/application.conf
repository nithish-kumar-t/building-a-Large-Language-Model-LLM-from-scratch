local {
  fs.defaultFS = "hdfs://localhost:9000"
  mapreduce.job.reduces = 1
  mapreduce.input.fileinputformat.split.maxsize = 67108864 // 64 MB
  inputPath = "/input/input.txt"
  outputPath = "/output/"
}

test {
  fs.defaultFS = "local"
  //  mapreduce.job.maps = 1
  mapreduce.job.reduces = 1
  mapreduce.input.fileinputformat.split.maxsize = 64000000 // 64 MB
  inputPath = "/Users/tnithish/Learning/cs-401/building-a-Large-Language-Model-LLM-from-scratch/src/main/resources/input.txt"
  outputPath = "/Users/tnithish/Learning/cs-401/building-a-Large-Language-Model-LLM-from-scratch/src/main/resources/output/"
}

cloud {
//  fs.defaultFS = "hdfs://localhost:9000" //AWS will by default take the fs.
  mapreduce.job.reduces = 3
  mapreduce.input.fileinputformat.split.maxsize = 256000000 // 256 MB
  inputPath  = "s3://hw1-training-llm/input/"
  outputPath = "s3://hw1-training-llm/output/"
}

epochs = "75"


//  mapred.input.format.class = "org.apache.hadoop.mapred.TextInputFormat"
//  mapred.output.format.class = "org.apache.hadoop.mapred.TextOutputFormat"